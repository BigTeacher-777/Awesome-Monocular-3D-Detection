# Awesome Monocular 3D detection
Paper list of 3D detetction, keep updating!

## Contents
- [Paper List](#Paper-List)
    - [2022](#2022)
    - [2021](#2021)
    - [2020](#2020)
    - [2019](#2019)
    - [2018](#2018)
    - [2017](#2017)
    - [2016](#2016)
- [KITTI Results](#KITTI-Results)

# Paper List
## 2022
- <a id='MonoCon'></a>**[MonoCon]** Learning Auxiliary Monocular Contexts Helps Monocular 3D Object Detection [[AAAI2022](https://arxiv.org/pdf/2112.04628.pdf)]


## 2021
- <a id="PCT"></a>**[PCT]** Progressive Coordinate Transforms for Monocular 3D Object Detection [[NeurIPS2021](https://arxiv.org/pdf/2108.05793.pdf)][[Pytorch](https://github.com/amazon-research/progressive-coordinate-transforms)]
- <a id="DFR-Net"></a>**[DFR-Net]** The Devil Is in the Task: Exploiting Reciprocal Appearance-Localization Features for Monocular 3D Object Detection [[ICCV2021](https://openaccess.thecvf.com/content/ICCV2021/html/Zou_The_Devil_Is_in_the_Task_Exploiting_Reciprocal_Appearance-Localization_Features_ICCV_2021_paper.html)]
- <a id="AutoShape"></a>**[AutoShape]** AutoShape: Real-Time Shape-Aware Monocular 3D Object Detection [[ICCV2021](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_AutoShape_Real-Time_Shape-Aware_Monocular_3D_Object_Detection_ICCV_2021_paper.pdf)][[Pytorch](https://github.com/zongdai/AutoShape)][[Paddle](https://github.com/zongdai/AutoShape)]
- <a id="analysis"></a>**[pseudo-analysis]** Are we Missing Confidence in Pseudo-LiDAR Methods for Monocular 3D Object Detection? [[ICCV2021](https://openaccess.thecvf.com/content/ICCV2021/papers/Simonelli_Are_We_Missing_Confidence_in_Pseudo-LiDAR_Methods_for_Monocular_3D_ICCV_2021_paper.pdf)]
- <a id="Gated3D"></a>**[Gated3D]** Gated3D: Monocular 3D Object Detection From Temporal Illumination Cues [[ICCV2021](https://openaccess.thecvf.com/content/ICCV2021/papers/Julca-Aguilar_Gated3D_Monocular_3D_Object_Detection_From_Temporal_Illumination_Cues_ICCV_2021_paper.pdf)]
- <a id="MonoRCNN"></a>**[MonoRCNN]** Geometry-based Distance Decomposition for Monocular 3D Object Detection [[ICCV2021](https://arxiv.org/abs/2104.03775)][[Pytorch](https://github.com/Rock-100/MonoDet)]
- <a id="DD3D"></a>**[DD3D]** Is Pseudo-Lidar needed for Monocular 3D Object detection [[ICCV2021](https://arxiv.org/pdf/2108.06417.pdf)][[Pytorch](https://github.com/tri-ml/dd3d)]
- <a id="GUPNet"></a>**[GUPNet]** Geometry Uncertainty Projection Network for Monocular 3D Object Detection [[ICCV2021](https://arxiv.org/pdf/2107.13774.pdf)][[Pytorch](https://github.com/SuperMHP/GUPNet)]
- <a id="neighbor-voting"></a>**[Neighbor-Vote]** Neighbor-Vote: Improving Monocular 3D Object Detection through Neighbor Distance Voting [[ACMMM2021](https://arxiv.org/pdf/2107.02493.pdf)]
- <a id="MonoEF"></a>**[MonoEF]** Monocular 3D Object Detection: An Extrinsic Parameter Free Approach [[CVPR2021](https://arxiv.org/abs/2106.15796?context=cs)][[Pytorch](https://github.com/ZhouYunsong-SJTU/MonoEF)]
- <a id="monodle"></a>**[monodle]** Delving into Localization Errors for Monocular 3D Object Detection [[CVPR2021](https://arxiv.org/abs/2103.16237)][[Pytorch](https://github.com/xinzhuma/monodle)]
- <a id="Monoflex"></a>**[Monoflex]** Objects are Different: Flexible Monocular 3D Object Detection [[CVPR2021](https://arxiv.org/abs/2104.02323)][[Pytorch](https://github.com/zhangyp15/MonoFlex)]
- <a id="GrooMeD-NMS"></a>**[GrooMeD-NMS]** GrooMeD-NMS: Grouped Mathematically Differentiable NMS for Monocular 3D Object Detection [[CVPR2021](https://arxiv.org/abs/2103.17202)][[Pytorch](https://github.com/abhi1kumar/groomed_nms)]
- <a id="DDMP-3D"></a>**[DDMP-3D]** Depth-conditioned Dynamic Message Propagation for Monocular 3D Object Detection [[CVPR2021](https://arxiv.org/abs/2103.16470)][[Pytorch](https://github.com/Willy0919/DDMP-3D)]
- <a id="MonoRUn"></a>**[MonoRUn]** MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation [[CVPR2021](https://arxiv.org/abs/2103.12605)][[Pytorch](https://github.com/tjiiv-cprg/MonoRUn)]
- <a id="M3DSSD"></a>**[M3DSSD]** M3DSSD: Monocular 3D Single Stage Object Detector [[CVPR2021](https://arxiv.org/abs/2103.13164)][[Pytorch](https://github.com/mumianyuxin/M3DSSD)]
- <a id="CaDDN"></a>**[CaDDN]** Categorical Depth Distribution Network for Monocular 3D Object Detection [[CVPR2021](https://arxiv.org/abs/2103.01100)][[Pytorch](https://github.com/TRAILab/CaDDN)]
- <a id="visualDet3D"></a>**[visualDet3D]** Ground-aware Monocular 3D Object Detection for Autonomous Driving [[RA-L](https://arxiv.org/abs/2102.00690)][[Pytorch](https://github.com/Owen-Liuyuxuan/visualDet3D)]
 
## 2020
- <a name="UR3D"></a>**[UR3D]** Distance-Normalized Unified Representation for Monocular 3D Object Detection [[ECCV2020](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123740086.pdf)]
- <a name="MonoDR"></a>**[MonoDR]** Monocular Differentiable Rendering for Self-Supervised 3D Object Detection [[ECCV2020](https://arxiv.org/abs/2009.14524)]
- <a id="DA-3Ddet"></a>**[DA-3Ddet]** Monocular 3d object detection via feature domain adaptation [[ECCV2020](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123540018.pdf)]
- <a id="MoVi-3D"></a>**[MoVi-3D]** Towards generalization across depth for monocular 3d object detection [[ECCV2020](https://arxiv.org/abs/1912.08035)]
- <a id="PatchNet"></a>**[PatchNet]** Rethinking Pseudo-LiDAR Representation [[ECCV2020](https://arxiv.org/abs/2008.04582)][[Pytorch](https://github.com/xinzhuma/patchnet)]
- <a id="RAR-Net"></a>**[RAR-Net]** Reinforced Axial Refinement Network for Monocular 3D Object Detection [[ECCV2020](https://arxiv.org/abs/2008.13748)]
- <a id='kinematic3d'></a>**[kinematic3d]** Kinematic 3D Object Detection in Monocular Video [[ECCV2020](https://arxiv.org/abs/2007.09548)][[Pytorch](https://github.com/garrickbrazil/kinematic3d)]
- <a id="RTM3D"></a>**[RTM3D]** RTM3D: Real-time Monocular 3D Detection from Object Keypoints for Autonomous Driving [[ECCV2020](https://arxiv.org/abs/2001.03343)][[Pytorch](https://github.com/Banconxuan/RTM3D)]
- <a id="SMOKE"></a>**[SMOKE]** SMOKE: Single-Stage Monocular 3D Object Detection via Keypoint Estimation [[CVPRW2020](https://arxiv.org/pdf/2002.10111.pdf)][[Pytorch](https://github.com/lzccccc/SMOKE)]
- <a id="D4LCN"></a>**[D4LCN]** Learning Depth-Guided Convolutions for Monocular 3D Object Detection [[CVPRW2020](https://arxiv.org/abs/1912.04799)][[Pytorch](https://github.com/dingmyu/D4LCN)]
- <a id="MonoPair"></a>**[MonoPair]** MonoPair: Monocular 3D Object Detection Using Pairwise Spatial Relationships [[CVPR2020](https://arxiv.org/abs/2003.00504)]
- <a id="pseudo-LiDAR_e2e"></a>**[pseudo-LiDAR_e2e]** End-to-End Pseudo-LiDAR for Image-Based 3D Object Detection [[CVPR2020](https://arxiv.org/abs/2004.03080)][[Pytorch](https://github.com/mileyan/pseudo-LiDAR_e2e)]
- <a id="Decoupled-3D"></a>**[Decoupled-3D]** Monocular 3D Object Detection with Decoupled Structured Polygon Estimation and Height-Guided Depth Estimation [[AAAI2020](https://arxiv.org/abs/2002.01619)]
- <a id="OACV"></a>**[OACV]** Object-Aware Centroid Voting for Monocular 3D Object Detection [[IROS2020](https://arxiv.org/abs/2007.09836)]
- <a id="ForeSeE"></a>**[ForeSeE]** Task-Aware Monocular Depth Estimation for 3D Object Detection [[AAAI2020(oral)](https://arxiv.org/abs/1909.07701)][[Pytorch](https://github.com/WXinlong/ForeSeE)]

## 2019
- <a id="Mono3DPLiDAR"></a>**[Mono3DPLiDAR]** Monocular 3D Object Detection with Pseudo-LiDAR Point Cloud [[ICCVW2019](https://arxiv.org/abs/1903.09847)]
- <a id="MonoDIS"></a>**[MonoDIS]** Disentangling monocular 3d object detection [[ICCV2019](https://openaccess.thecvf.com/content_ICCV_2019/papers/Simonelli_Disentangling_Monocular_3D_Object_Detection_ICCV_2019_paper.pdf)]
- <a id="AM3D"></a>**[AM3D]** Accurate Monocular Object Detection via Color-Embedded 3D Reconstruction for Autonomous Driving [[ICCV2019](https://arxiv.org/abs/1903.11444)]
- <a id="M3D-RPN"></a>**[M3D-RPN]** M3D-RPN: Monocular 3D Region Proposal Network for Object Detection [[ICCV2019(Oral)](https://arxiv.org/abs/1907.06038)][[Pytorch](https://github.com/garrickbrazil/M3D-RPN)]
- <a id="MVRA"></a>**[MVRA]** Multi-View Reprojection Architecture for Orientation Estimation [[ICCVW2019](https://openaccess.thecvf.com/content_ICCVW_2019/papers/ADW/Choi_Multi-View_Reprojection_Architecture_for_Orientation_Estimation_ICCVW_2019_paper.pdf)]
- <a id="MonoPSR"></a>**[MonoPSR]** Monocular 3D Object Detection Leveraging Accurate Proposals and Shape Reconstruction [[CVPR2019](https://arxiv.org/abs/1904.01690)][[Pytorch](https://github.com/kujason/monopsr)]
- <a id="FQNet"></a>**[FQNet]** Deep fitting degree scoring network for monocular 3d object detection [[CVPR2019](https://arxiv.org/abs/1904.12681)]
- <a id="ROI-10D"></a>**[ROI-10D]** ROI-10D: Monocular Lifting of 2D Detection to 6D Pose and Metric Shape [[CVPR2019](https://arxiv.org/abs/1812.02781)]
- <a id="GS3D"></a>**[GS3D]** GS3D: An Efficient 3D Object Detection Framework for Autonomous Driving [[CVPR2019](https://openaccess.thecvf.com/content_CVPR_2019/html/Li_GS3D_An_Efficient_3D_Object_Detection_Framework_for_Autonomous_Driving_CVPR_2019_paper.html)]
- <a id="MonoGRNet"></a>**[MonoGRNet]** MonoGRNet: A Geometric Reasoning Network for Monocular 3D Object Localization [[AAAI2019(oral)](https://arxiv.org/abs/1811.10247)][[Tensorflow](https://github.com/Zengyi-Qin/MonoGRNet)]
- <a id="OFT-Net"></a>**[OFT-Net]** Orthographic feature transform for monocular 3d object detection [[BMVC2019](https://bmvc2019.org/wp-content/uploads/papers/0328-paper.pdf)][[Pytorch](https://github.com/tom-roddick/oft)]
- <a id="Shift R-CNN"></a>**[Shift R-CNN]** Shift R-CNN: Deep Monocular 3D Object Detection with Closed-Form Geometric Constraints [[TIP2019](https://arxiv.org/abs/1905.09970)]
- <a id="SS3D"></a>**[SS3D]** SS3D: Monocular 3d object detection and box fitting trained end-to-end using intersection-over-union loss [[Arxiv2019](https://arxiv.org/abs/1906.08070)]

## 2018
- <a id="Multi-Fusion"></a>**[Multi-Fusion]** Multi-Level Fusion based 3D Object Detection from Monocular Images [[CVPR2018](https://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_Multi-Level_Fusion_Based_CVPR_2018_paper.pdf)][[Pytorch](https://github.com/abbyxxn/maskrcnn-benchmark-3d)]
- <a id="Mono3D++"></a>**[Mono3D++]** Mono3D++: Monocular 3D Vehicle Detection with Two-Scale 3D Hypotheses and Task Priors [[AAAI2018](https://arxiv.org/abs/1901.03446)]

## 2017
- <a id="Deep3DBox"></a>**[Deep3DBox]** 3D Bounding Box Estimation Using Deep Learning and Geometry [[CVPR2017](https://arxiv.org/abs/1612.00496)][[Pytorch](https://github.com/skhadem/3D-BoundingBox)][[Tensorflow](https://github.com/smallcorgi/3D-Deepbox)]
- <a id="Deep MANTA"></a>**[Deep MANTA]** Deep MANTA: A Coarse-to-fine Many-Task Network for joint 2D and 3D vehicle analysis from monocular image [[CVPR2017](https://arxiv.org/abs/1703.07570)]

## 2016
- <a id="Mono3D"></a>**[Mono3D]** Monocular 3D object detection for autonomous driving [[CVPR2016](https://www.cs.toronto.edu/~urtasun/publications/chen_etal_cvpr16.pdf)]



# KITTI Results
<!-- <font color=blue, size=4>val/test</font><font color=blue, size=3> (R<sub>11</sub>/R<sub>40</sub>) @ IOU=0.7</font> -->
<!-- <style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:5px 10px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-9wq8{border-color:inherit;text-align:center;vertical-align:middle}
.tg .tg-nrix{border-color:inherit;text-align:center;vertical-align:middle}
</style> -->
<table class="tg" style="text-align:center">
<thead>
  <tr>
    <th class="tg-9wq8" rowspan="2">Method</th>
    <th class="tg-nrix" rowspan="2">Extra</th>
    <th class="tg-nrix" colspan="3">Test,      
    AP<sub>3D</sub>|<sub>R<sub>40</sub></th>
    <th class="tg-nrix" colspan="3">Val,      
    AP<sub>3D</sub>|<sub>R<sub>40</sub></th>
      <th class="tg-nrix" colspan="3">Val, 
    AP<sub>3D</sub>|<sub>R<sub>11</sub></th>
    <th class="tg-nrix" rowspan="2">Reference</th>
  </tr>
  <tr>
    <th class="tg-nrix">Easy</th>
    <th class="tg-nrix">Mod.</th>
    <th class="tg-nrix">Hard</th>
    <th class="tg-nrix">Easy</th>
    <th class="tg-nrix">Mod.</th>
    <th class="tg-nrix">Hard</th>
    <th class="tg-nrix">Easy</th>
    <th class="tg-nrix">Mod.</th>
    <th class="tg-nrix">Hard</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-9wq8" nowrap="nowrap"><a href="#M3D-RPN">M3D-RPN</a></td>
    <td class="tg-nrix">None</td>
    <td class="tg-nrix">14.76</td>
    <td class="tg-nrix">9.71</td>
    <td class="tg-nrix">7.42</td>
    <td class="tg-nrix">14.53</td>
    <td class="tg-nrix">11.07</td>
    <td class="tg-nrix">8.65</td>
    <td class="tg-nrix">20.27</td>
    <td class="tg-nrix">17.06</td>
    <td class="tg-nrix">15.21</td>
    <td class="tg-nrix">ICCV2019</td>
  </tr>
  <tr>
    <td class="tg-9wq8"><a href="#SMOKE">SMOKE</a></td>
    <td class="tg-nrix">None</td>
    <td class="tg-nrix">14.03</td>
    <td class="tg-nrix">9.76</td>
    <td class="tg-nrix">7.84 </td>
    <td class="tg-nrix">-</td>
    <td class="tg-nrix">-</td>
    <td class="tg-nrix">-</td>
    <td class="tg-nrix">14.76</td>
    <td class="tg-nrix">12.85</td>
    <td class="tg-nrix">11.50</td>
    <td class="tg-nrix">CVPRW2020</td>
  </tr>
  <tr>
    <td class="tg-9wq8"><a href="#MonoPair">MonoPair</a></td>
    <td class="tg-nrix">None</td>
    <td class="tg-nrix">13.04</td>
    <td class="tg-nrix">9.99</td>
    <td class="tg-nrix">8.65 </td>
    <td class="tg-nrix">16.28</td>
    <td class="tg-nrix">12.30</td>
    <td class="tg-nrix">10.42</td>
    <td class="tg-nrix">-</td>
    <td class="tg-nrix">-</td>
    <td class="tg-nrix">-</td>
    <td class="tg-nrix">CVPR2020</td>
  </tr>
  <tr>
    <td class="tg-9wq8"><a href="#RTM3D">RTM3D</a></td>
    <td class="tg-nrix">None</td>
    <td class="tg-nrix">14.41</td>
    <td class="tg-nrix">10.34</td>
    <td class="tg-nrix">8.77 </td>
    <td class="tg-nrix">-</td>
    <td class="tg-nrix">-</td>
    <td class="tg-nrix">-</td>
    <td class="tg-nrix">20.77</td>
    <td class="tg-nrix">16.86</td>
    <td class="tg-nrix">16.63</td>
    <td class="tg-nrix">ECCV2020</td>
  </tr>
  <tr>
    <td class="tg-9wq8"><a href="#M3DSSD">M3DSSD</a></td>
    <td class="tg-nrix">None</td>
    <td class="tg-nrix">17.51</td>
    <td class="tg-nrix">11.46</td>
    <td class="tg-nrix">8.98 </td>
    <td class="tg-nrix">-</td>
    <td class="tg-nrix">-</td>
    <td class="tg-nrix">-</td>
    <td class="tg-nrix">27.77</td>
    <td class="tg-nrix">21.67</td>
    <td class="tg-nrix">18.28</td>
    <td class="tg-nrix">CVPR2021</td>
  </tr>
  <tr>
    <td class="tg-9wq8"><a href="#Monoflex">Monoflex</a></td>
    <td class="tg-nrix">None</td>
    <td class="tg-nrix">19.94</td>
    <td class="tg-nrix">13.89</td>
    <td class="tg-nrix">12.07 </td>
    <td class="tg-nrix">23.64</td>
    <td class="tg-nrix">17.51</td>
    <td class="tg-nrix">14.83</td>
    <td class="tg-nrix">28.17</td>
    <td class="tg-nrix">21.92</td>
    <td class="tg-nrix">19.07</td>
    <td class="tg-nrix">CVPR2021</td>
  </tr>
  <tr>
    <td class="tg-9wq8"><a href="#GUPNet">GUPNet</a></td>     
    <td class="tg-nrix">None</td>     
    <td class="tg-nrix">20.11</td>
    <td class="tg-nrix">14.20</td>
    <td class="tg-nrix">11.77</td>
    <td class="tg-nrix">22.76</td>
    <td class="tg-nrix">16.46</td>
    <td class="tg-nrix">13.72</td>
    <td class="tg-nrix">-</td>
    <td class="tg-nrix">-</td>
    <td class="tg-nrix">-</td>
    <td class="tg-nrix">ICCV2021</td>
  </tr>
  <tr>
    <td class="tg-9wq8"><a href="#MonoCon">MonoCon</a></td>     
    <td class="tg-nrix">None</td>     
    <td class="tg-nrix">22.50</td>
    <td class="tg-nrix">16.46</td>
    <td class="tg-nrix">13.95</td>
    <td class="tg-nrix">26.33</td>
    <td class="tg-nrix">19.01</td>
    <td class="tg-nrix">15.98</td>
    <td class="tg-nrix">-</td>
    <td class="tg-nrix">-</td>
    <td class="tg-nrix">-</td>
    <td class="tg-nrix">AAAI2022</td>
  </tr>
  <tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
  <tr>
    <td class="tg-9wq8"><a href="#AM3D">AM3D</a></td>
    <td class="tg-nrix">Depth</td>     
    <td class="tg-nrix">16.50</td>
    <td class="tg-nrix">10.74</td>
    <td class="tg-nrix">9.52</td>
    <td class="tg-nrix">28.31</td>
    <td class="tg-nrix">15.76</td>
    <td class="tg-nrix">12.24</td>
    <td class="tg-nrix">32.23</td>
    <td class="tg-nrix">21.09</td>
    <td class="tg-nrix">17.26</td>
    <td class="tg-nrix">ICCV2019</td>
    </tr>
  <tr>
    <td class="tg-9wq8"><a href="#PatchNet">PatchNet</a></td>     
    <td class="tg-nrix">Depth</td>     
    <td class="tg-nrix">15.68</td>
    <td class="tg-nrix">11.12</td>
    <td class="tg-nrix">10.17</td>
    <td class="tg-nrix">31.60</td>
    <td class="tg-nrix">16.80</td>
    <td class="tg-nrix">13.80</td>
    <td class="tg-nrix">35.10</td>
    <td class="tg-nrix">22.00</td>
    <td class="tg-nrix">19.60</td>
    <td class="tg-nrix">ECCV2020</td>
  </tr>
    <tr>
    <td class="tg-9wq8"><a href="#D4LCN">D4LCN</a></td>     
    <td class="tg-nrix">Depth</td>     
    <td class="tg-nrix">16.65</td>
    <td class="tg-nrix">11.72</td>
    <td class="tg-nrix">9.51</td>
    <td class="tg-nrix">22.32</td>
    <td class="tg-nrix">16.20</td>
    <td class="tg-nrix">12.30</td>
    <td class="tg-nrix">26.97</td>
    <td class="tg-nrix">21.72</td>
    <td class="tg-nrix">18.22</td>
    <td class="tg-nrix">CVPRW2020</td>
  </tr>
  <tr>
    <td class="tg-9wq8"><a href="#CaDDN">CaDDN</a></td>     
    <td class="tg-nrix">Depth</td>     
    <td class="tg-nrix">19.17</td>
    <td class="tg-nrix">13.41</td>
    <td class="tg-nrix">11.46</td>
    <td class="tg-nrix">23.57</td>
    <td class="tg-nrix">16.31</td>
    <td class="tg-nrix">13.84</td>
    <td class="tg-nrix">-</td>
    <td class="tg-nrix">-</td>
    <td class="tg-nrix">-</td>
    <td class="tg-nrix">CVPR2021</td>
  </tr>
  <tr>
    <td class="tg-9wq8"><a href="#DFR-Net">DFR-Net</a></td>     
    <td class="tg-nrix">Depth</td>     
    <td class="tg-nrix">19.40</td>
    <td class="tg-nrix">13.63</td>
    <td class="tg-nrix">10.35</td>
    <td class="tg-nrix">24.81</td>
    <td class="tg-nrix">17.78</td>
    <td class="tg-nrix">14.41</td>
    <td class="tg-nrix">28.80</td>
    <td class="tg-nrix">22.88</td>
    <td class="tg-nrix">19.47</td>
    <td class="tg-nrix">ICCV2021</td>
  </tr>



  
      
</tbody>
</table>




